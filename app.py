import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from imblearn.over_sampling import SMOTE  # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡

try:
    kodak = pd.read_csv('kodak.txt', sep='\t', header=None)
    fuji = pd.read_csv('fuji.txt', sep='\t', header=None)
    agfa = pd.read_csv('agfa.txt', sep='\t', header=None)
    unknown = pd.read_csv('inconnu.txt', sep='\t', header=None)
    print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ù†Ø¬Ø§Ø­.")
except Exception as e:
    print(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª: {e}")  # Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ ÙƒÙ„ Ù…Ù„Ù Ù‚Ø¨Ù„ Ø§Ù„Ø¯Ù…Ø¬
print("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ù…Ù„Ù Fuji:", fuji.shape[1])  # Ù‡Ø°Ø§ Ø³ÙŠØ·Ø¨Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ù…Ù„Ù Fuji
print("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ù…Ù„Ù Agfa:", agfa.shape[1])  # Ù‡Ø°Ø§ Ø³ÙŠØ·Ø¨Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ù…Ù„Ù Agfa
print("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ù…Ù„Ù Kodak:", kodak.shape[1])  # Ù‡Ø°Ø§ Ø³ÙŠØ·Ø¨Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ù…Ù„Ù Kodak
print("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ù…Ù„Ù unknown:", unknown.shape[1])

# Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± ÙÙŠ ÙƒÙ„ Ù…Ù„Ù Ù‚Ø¨Ù„ Ø§Ù„Ø¯Ù…Ø¬
print("Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± ÙÙŠ Ù…Ù„Ù Fuji:", fuji.shape[0])  # Ù‡Ø°Ø§ Ø³ÙŠØ·Ø¨Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± ÙÙŠ Ù…Ù„Ù Fuji
print("Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± ÙÙŠ Ù…Ù„Ù Agfa:", agfa.shape[0])  # Ù‡Ø°Ø§ Ø³ÙŠØ·Ø¨Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± ÙÙŠ Ù…Ù„Ù Agfa
print("Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± ÙÙŠ Ù…Ù„Ù Kodak:", kodak.shape[0])  # Ù‡Ø°Ø§ Ø³ÙŠØ·Ø¨Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± ÙÙŠ Ù…Ù„Ù Kodak
print("Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± ÙÙŠ Ù…Ù„Ù unknown:", unknown.shape[0])  # Ù‡Ø°Ø§ Ø³ÙŠØ·Ø¨Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± ÙÙŠ Ù…Ù„Ù unknown

# Ø¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙØ¦Ø© Ù„ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª
fuji['Label'] = 'Fuji'
agfa['Label'] = 'Agfa'
kodak['Label'] = 'Kodak'

# Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù„Ù ÙˆØ§Ø­Ø¯
data = pd.concat([fuji, agfa, kodak], axis=0, ignore_index=True)
# Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print("\nÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
print(data.info())

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
print("\nØ¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©:")
print(data.isnull().sum())

# ØªØ·Ø¨ÙŠÙ‚ SMOTE Ù„Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„ÙØ¦Ø§Øª
print("\nØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù„ÙƒÙ„ ÙØ¦Ø© Ù‚Ø¨Ù„ SMOTE:")
print(data['Label'].value_counts())
X = data.iloc[:, :-1].values
y = data['Label'].values

smote = SMOTE(
    sampling_strategy={'Fuji': 100, 'Agfa': 119, 'Kodak': 128},
    random_state=42
)
X_resampled, y_resampled = smote.fit_resample(X, y)
# Ø¥Ù†Ø´Ø§Ø¡ DataFrame Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ø¯ ØªÙˆØ§Ø²Ù†Ù‡Ø§
balanced_data = pd.DataFrame(X_resampled)
balanced_data['Label'] = y_resampled

print("\nØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù„ÙƒÙ„ ÙØ¦Ø© Ø¨Ø¹Ø¯ SMOTE:")
print(balanced_data['Label'].value_counts())
# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØ§Ù„ØªØ³Ù…ÙŠØ§Øª Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ø¯ ØªÙˆØ§Ø²Ù†Ù‡Ø§
X = balanced_data.iloc[:, :-1].values
y = balanced_data['Label'].values
# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print("\nØ­Ø¬Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨:", len(X_train))
print("Ø­Ø¬Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:", len(X_test))  # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
imputer = SimpleImputer(strategy="mean")
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)  # ØªÙ‚ÙŠÙŠØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ø­ÙØ¸ Ø§Ù„Ù€ scaler
joblib.dump(scaler, 'scaler.pkl')


def plot_accuracy_comparison(accuracies, title):
    plt.figure(figsize=(8, 5))
    names = list(accuracies.keys())
    values = list(accuracies.values())
    sns.barplot(x=names, y=values, palette="viridis")
    plt.title(title)
    plt.ylabel("Ø§Ù„Ø¯Ù‚Ø© (%)")
    plt.xlabel("Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ©")
    plt.ylim(0, 100)  # Ù„Ø¶Ù…Ø§Ù† ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³
    plt.show()


# Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ
classifiers = {
    "Decision Tree (CART)": DecisionTreeClassifier(random_state=42),
    "LDA": LDA(),
    "QDA": QuadraticDiscriminantAnalysis(),
    "SVM": SVC(kernel='linear', random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Ù…ØªØºÙŠØ±Ø§Øª Ù„ØªØ®Ø²ÙŠÙ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
best_accuracy = 0
best_model = None
best_model_name = ""

# Ø­ÙØ¸ Ø§Ù„Ø¯Ù‚Ø© Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
accuracies_original = {}
accuracies_pca = {}
accuracies_lda = {}

# Ø§Ù„ØªØµÙ†ÙŠÙ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
print("\nÙ†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:")
for name, clf in classifiers.items():
    clf.fit(X_train_scaled, y_train)
    y_pred = clf.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred) * 100
    accuracies_original[name] = accuracy
    print(f"{name} - Ø§Ù„Ø¯Ù‚Ø©: {accuracy:.2f}%")
    print("Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ:\n", confusion_matrix(y_test, y_pred), "\n")

    # ØªØ­Ø¯ÙŠØ« Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = clf
        best_model_name = f"{name} (Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©)"

# Ø±Ø³Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¯Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
plot_accuracy_comparison(accuracies_original, "Ù…Ù‚Ø§Ø±Ù†Ø© Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©")  # Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø¹Ø¯ PCA
pca = PCA(n_components=50)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# ØªØ¯Ø±ÙŠØ¨ LDA Ø¨Ø¹Ø¯ PCA
lda = LDA()
lda.fit(X_train_pca, y_train)

# Ø­ÙØ¸ LDA ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø®Ø±Ù‰
joblib.dump(lda, 'best_lda.pkl')
joblib.dump(pca, 'best_pca.pkl')

print("\nÙ†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø¹Ø¯ PCA:")
for name, clf in classifiers.items():
    clf.fit(X_train_pca, y_train)
    y_pred = clf.predict(X_test_pca)
    accuracy = accuracy_score(y_test, y_pred) * 100
    accuracies_pca[name] = accuracy
    print(f"{name} - Ø§Ù„Ø¯Ù‚Ø© Ø¨Ø¹Ø¯ PCA: {accuracy:.2f}%")
    print("Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ:\n", confusion_matrix(y_test, y_pred), "\n")

    # ØªØ­Ø¯ÙŠØ« Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
    if accuracy > best_accuracy or (accuracy == 100.00 and name == 'LDA'):  # ØªØ®ØµÙŠØµ LDA ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ³Ø§ÙˆÙŠ
        best_accuracy = accuracy
        best_model = clf
        best_model_name = f"{name} (Ø¨Ø¹Ø¯ PCA)"

# Ø±Ø³Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¯Ù‚Ø© Ø¨Ø¹Ø¯ PCA
plot_accuracy_comparison(accuracies_pca, "Ù…Ù‚Ø§Ø±Ù†Ø© Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ - Ø¨Ø¹Ø¯ PCA")

# Ø­ÙØ¸ LDA Ø¨Ø¹Ø¯ PCA Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡ÙŠ Ø§Ù„Ø£ÙØ¶Ù„
lda_after_pca = LDA()
lda_after_pca.fit(X_train_pca, y_train)
joblib.dump(lda_after_pca, 'best_lda_after_pca.pkl')
print("âœ… ØªÙ… Ø­ÙØ¸ LDA Ø¨Ø¹Ø¯ PCA: best_lda_after_pca.pkl")

# Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù€ LDA
if best_model_name == "LDA (Ø¨Ø¹Ø¯ PCA)":
    joblib.dump(best_model, "best_model.pkl")
    print(f"âœ… ØªÙ… Ø­ÙØ¸ LDA ÙƒØ£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model_name}")
elif best_model_name == "QDA (Ø¨Ø¹Ø¯ PCA)":
    joblib.dump(best_model, "best_model.pkl")
    print(f"âœ… ØªÙ… Ø­ÙØ¸ QDA ÙƒØ£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model_name}")

joblib.dump(scaler, 'scaler.pkl')
print(f"\nâœ… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model_name} - Ø¯Ù‚Ø©: {best_accuracy:.2f}%")
print("- ØªÙ… Ø­ÙØ¸ best_model.pkl Ùˆ scaler.pkl")

if "(Ø¨Ø¹Ø¯ PCA)" in best_model_name:
    joblib.dump(pca, 'best_pca.pkl')
    print("- ØªÙ… Ø­ÙØ¸ PCA: best_pca.pkl")

if "(Ø¨Ø¹Ø¯ FDA)" in best_model_name:
    joblib.dump(top_6_indices, 'best_fda_indices.pkl')
    joblib.dump(fisher_scores, 'fda_scores.pkl')
    print("- ØªÙ… Ø­ÙØ¸ Ù…Ù„ÙØ§Øª FDA")


# 1. Ø­Ø³Ø§Ø¨ Fisher Scores Ù„ÙƒÙ„ Ø·ÙˆÙ„ Ù…ÙˆØ¬ÙŠ
def calculate_fisher_scores(X, y):
    classes = np.unique(y)
    n_features = X.shape[1]
    fisher_scores = np.zeros(n_features)

    for i in range(n_features):
        overall_mean = np.mean(X[:, i])
        numerator = 0
        denominator = 0

        for c in classes:
            X_c = X[y == c, i]
            mean_c = np.mean(X_c)
            var_c = np.var(X_c, ddof=1)

            n_c = len(X_c)
            numerator += n_c * (mean_c - overall_mean) ** 2
            denominator += (n_c - 1) * var_c

        fisher_scores[i] = numerator / denominator if denominator != 0 else 0

    return fisher_scores


# 2. Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Fisher ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙØ¶Ù„ 6 Ø£Ø·ÙˆØ§Ù„
fisher_scores = calculate_fisher_scores(X_train_scaled, y_train)
top_6_indices = np.argsort(fisher_scores)[-6:][::-1]  # Ø£ÙØ¶Ù„ 6 Ø£Ø·ÙˆØ§Ù„
print("Ø£ÙØ¶Ù„ 6 Ø£Ø·ÙˆØ§Ù„ Ù…ÙˆØ¬ÙŠØ© (Fisher Score):", top_6_indices)

# Ø­ÙØ¸ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙˆØ¯Ø±Ø¬Ø§Øª Fisher
joblib.dump(top_6_indices, 'best_fda_indices.pkl')
joblib.dump(fisher_scores, 'fda_scores.pkl')

# 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø© ÙÙ‚Ø·
X_train_fisher = X_train_scaled[:, top_6_indices]
X_test_fisher = X_test_scaled[:, top_6_indices]

# 4. ØªØ·Ø¨ÙŠÙ‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø·ÙˆØ§Ù„ Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
results = {}
for name, clf in classifiers.items():
    clf.fit(X_train_fisher, y_train)
    y_pred = clf.predict(X_test_fisher)
    accuracy = accuracy_score(y_test, y_pred) * 100
    cm = confusion_matrix(y_test, y_pred)
    results[name] = {'accuracy': accuracy, 'confusion_matrix': cm}

    print(f"\n{name} - Ø§Ù„Ø¯Ù‚Ø© Ø¨Ø¹Ø¯ FDA: {accuracy:.2f}%")
    print("Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ:\n", cm)

# 5. Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ (Ù…Ø«Ù„ LDA Ø£Ùˆ QDA)
best_model_name = max(results, key=lambda x: results[x]['accuracy'])
best_model = classifiers[best_model_name]
joblib.dump(best_model, f'best_model_fisher_{best_model_name}.pkl')

# 6. Ø±Ø³Ù… Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¯Ù‚Ø©
plt.figure(figsize=(10, 6))
plt.bar(results.keys(), [results[name]['accuracy'] for name in results])
plt.title("Ù…Ù‚Ø§Ø±Ù†Ø© Ø¯Ù‚Ø© Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ FDA")
plt.ylabel("Ø§Ù„Ø¯Ù‚Ø© (%)")
plt.ylim(0, 100)
plt.xticks(rotation=45)
plt.show()  # Ø¨Ø¹Ø¯ Ù‚Ø³Ù…Ø© PCA ÙˆØ§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ù‚Ù… Ø¨Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ù„Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬

# ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ (LDA Ø¨Ø¹Ø¯ PCA)
best_model_name = "LDA (Ø¨Ø¹Ø¯ PCA)"
best_model = LDA()  # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ LDA Ø¬Ø¯ÙŠØ¯
best_model.fit(X_train_pca, y_train)  # ØªØ¯Ø±ÙŠØ¨Ù‡ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª PCA

# Ø­ÙØ¸ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©
import joblib

# 1. Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
joblib.dump(best_model, 'best_model.pkl')

# 2. Ø­ÙØ¸ PCA
joblib.dump(pca, 'pca_model.pkl')

# 3. Ø­ÙØ¸ Scaler
joblib.dump(scaler, 'scaler.pkl')

print("ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­:")
print("- best_model.pkl (Ù†Ù…ÙˆØ°Ø¬ LDA Ø§Ù„Ù…Ø¯Ø±Ø¨)")
print("- pca_model.pkl (Ù†Ù…ÙˆØ°Ø¬ PCA Ø§Ù„Ù…Ø¯Ø±Ø¨)")
print("- scaler.pkl (Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ·Ø¨ÙŠØ¹)")
import joblib
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
try:
    scaler = joblib.load("scaler.pkl")  # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠØ³
    pca = joblib.load("best_pca.pkl")  # Ù†Ù…ÙˆØ°Ø¬ PCA
    lda_model = joblib.load("best_model.pkl")  # Ù†Ù…ÙˆØ°Ø¬ LDA Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø¨Ø¹Ø¯ PCA
except Exception as e:
    raise ValueError(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")

# 2. ØªØ·Ø¨ÙŠÙ‚ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ø¨Ù†ÙØ³ ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¨)
X_test_scaled = scaler.transform(X_test)  # Ø§Ù„ØªÙ‚ÙŠÙŠØ³
X_test_pca = pca.transform(X_test_scaled)  # ØªØ·Ø¨ÙŠÙ‚ PCA

# 3. Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LDA
y_pred = lda_model.predict(X_test_pca)

# 4. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
accuracy = accuracy_score(y_test, y_pred) * 100
cm = confusion_matrix(y_test, y_pred)

# 5. Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
print("\n" + "=" * 50)
print(f"âœ… Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (LDA Ø¨Ø¹Ø¯ PCA): {accuracy:.2f}%")
print("\nğŸ“Š Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ:")
print(cm)
print("\nğŸ” Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:")
for true, pred in zip(y_test, y_pred):
    print(f"Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ: {true} â† Ø§Ù„Ù…ØªÙ†Ø¨Ø£: {pred}")
print("=" * 50)
import joblib
import numpy as np


def predict_unknown_samples(unknown_data):
    """
    ØªÙ‚ÙˆÙ… Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LDA Ø¨Ø¹Ø¯ PCA
    :param unknown_data: Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ© (DataFrame Ø£Ùˆ numpy array)
    :return: ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    """
    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
    try:
        print("\nğŸ” Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
        scaler = joblib.load("scaler.pkl")
        pca = joblib.load("best_pca.pkl")
        lda_model = joblib.load("best_model.pkl")
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ (Scaler, PCA, LDA)")
    except Exception as e:
        raise ValueError(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")

    # 2. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ numpy array Ø¥Ø°Ø§ ÙƒØ§Ù†Øª DataFrame
    if hasattr(unknown_data, 'values'):
        X_unknown = unknown_data.values
    else:
        X_unknown = np.array(unknown_data)

    # 3. ØªØ·Ø¨ÙŠÙ‚ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø¨Ù†ÙØ³ ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¨)
    try:
        print("\nâš™ï¸ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        X_unknown_scaled = scaler.transform(X_unknown)  # Ø§Ù„ØªÙ‚ÙŠÙŠØ³
        X_unknown_pca = pca.transform(X_unknown_scaled)  # ØªØ·Ø¨ÙŠÙ‚ PCA
        print("âœ… ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ‚ÙŠÙŠØ³ Ùˆ PCA Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        raise ValueError(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")

    # 4. Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LDA
    try:
        print("\nğŸ”® Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤...")
        y_pred = lda_model.predict(X_unknown_pca)
        print("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        raise ValueError(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")

    # 5. Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\nğŸ¯ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤:")
    print("=" * 40)
    for i, pred in enumerate(y_pred, 1):
        print(f"Ø§Ù„Ø¹ÙŠÙ†Ø© {i}: â†’ Ø§Ù„ÙØ¦Ø© {pred}")
    print("=" * 40)

    return y_pred


# ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
# predictions = predict_unknown_samples(unknown_data)
predictions = predict_unknown_samples(unknown) 